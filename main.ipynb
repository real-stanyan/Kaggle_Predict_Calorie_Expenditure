{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfe9b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置设备：支持 MPS（Apple Silicon）或 CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 创建模型保存目录\n",
    "os.makedirs(\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66c64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.clamp(y_pred, min=0)\n",
    "        log_pred = torch.log1p(y_pred)\n",
    "        log_true = torch.log1p(y_true)\n",
    "        return torch.sqrt(torch.mean((log_pred - log_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286c5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(file_path, save_scaler_path='model/scaler.pkl', test_size=0.2, random_state=42):\n",
    "    df = pd.read_csv(file_path).dropna()\n",
    "    df = df.drop(columns=['id'])\n",
    "    df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    x = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    joblib.dump(scaler, save_scaler_path)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        x_scaled, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    x_valid = x_valid.astype(np.float32)\n",
    "    y_valid = y_valid.astype(np.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train.values))\n",
    "    valid_dataset = TensorDataset(torch.tensor(x_valid), torch.tensor(y_valid.values))\n",
    "\n",
    "    return train_dataset, valid_dataset, x.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(dim, 256)\n",
    "        self.bn_input = nn.BatchNorm1d(256)\n",
    "\n",
    "        # 残差线性升维\n",
    "        self.res_proj1 = nn.Linear(256, 512)\n",
    "        self.res_proj2 = nn.Linear(512, 512)\n",
    "        self.res_proj3 = nn.Linear(512, 256)\n",
    "\n",
    "        # Block 1: 256 → 512\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(256, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Block 2: 512 → 512\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Block 3: 512 → 512\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Block 4: 512 → 256\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.SiLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256), nn.BatchNorm1d(256), nn.SiLU(), nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Block 5: 256 → 128\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.SiLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128), nn.BatchNorm1d(128), nn.SiLU(), nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Block 6: 128 → 64\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.SiLU(), nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Output block\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.SiLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.bn_input(self.input(x)))\n",
    "\n",
    "        res1 = self.res_proj1(x)\n",
    "        x = self.block1(x) + res1\n",
    "\n",
    "        res2 = self.res_proj2(x)\n",
    "        x = self.block2(x) + res2\n",
    "\n",
    "        res3 = x\n",
    "        x = self.block3(x) + res3\n",
    "\n",
    "        x = self.block4(x) + self.res_proj3(res3)\n",
    "\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.output_block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cad0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, valid_dataset, dim, num_epoch):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = Model(dim).to(device)\n",
    "    criterion = RMSLELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ===== 验证阶段 =====\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loader:\n",
    "                x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                total_valid_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / valid_batches\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "\n",
    "        scheduler.step(avg_valid_loss)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}, Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'model/model.pth')\n",
    "    print(\"模型已保存到 model/model.pth\")\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epoch + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epoch + 1), valid_losses, label='Valid Loss', marker='s')\n",
    "    plt.title('Training & Validation Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSLE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('model/loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def test(test_dataset, dim):\n",
    "    dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    model = Model(dim).to(device)\n",
    "    model.load_state_dict(torch.load('model/model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            predictions.extend(y_pred.cpu().squeeze().tolist())\n",
    "\n",
    "    print(\"测试集预测完成，前10个结果：\", predictions[:10])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e60017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.9719, Valid Loss: 0.9466, Time: 28.41s\n",
      "Epoch 2, Train Loss: 0.5394, Valid Loss: 0.1815, Time: 27.27s\n",
      "Epoch 3, Train Loss: 0.1955, Valid Loss: 0.1009, Time: 27.04s\n",
      "Epoch 4, Train Loss: 0.1648, Valid Loss: 0.0752, Time: 25.44s\n",
      "Epoch 5, Train Loss: 0.1573, Valid Loss: 0.0829, Time: 26.75s\n",
      "Epoch 6, Train Loss: 0.1528, Valid Loss: 0.0701, Time: 28.14s\n",
      "Epoch 7, Train Loss: 0.1501, Valid Loss: 0.0714, Time: 27.37s\n",
      "Epoch 8, Train Loss: 0.1462, Valid Loss: 0.0634, Time: 25.01s\n",
      "Epoch 9, Train Loss: 0.1440, Valid Loss: 0.0661, Time: 27.56s\n",
      "Epoch 10, Train Loss: 0.1420, Valid Loss: 0.0744, Time: 27.79s\n",
      "Epoch 11, Train Loss: 0.1407, Valid Loss: 0.0780, Time: 29.23s\n",
      "Epoch 12, Train Loss: 0.1391, Valid Loss: 0.0645, Time: 25.80s\n",
      "Epoch 13, Train Loss: 0.1383, Valid Loss: 0.0735, Time: 25.88s\n",
      "Epoch 14, Train Loss: 0.1369, Valid Loss: 0.0642, Time: 25.88s\n",
      "Epoch 15, Train Loss: 0.1355, Valid Loss: 0.0667, Time: 24.70s\n",
      "Epoch 16, Train Loss: 0.1345, Valid Loss: 0.0631, Time: 24.98s\n",
      "Epoch 17, Train Loss: 0.1336, Valid Loss: 0.0639, Time: 24.68s\n",
      "Epoch 18, Train Loss: 0.1329, Valid Loss: 0.0617, Time: 26.03s\n",
      "Epoch 19, Train Loss: 0.1314, Valid Loss: 0.0680, Time: 25.73s\n",
      "Epoch 20, Train Loss: 0.1311, Valid Loss: 0.0640, Time: 24.42s\n",
      "Epoch 21, Train Loss: 0.1302, Valid Loss: 0.0631, Time: 26.81s\n",
      "Epoch 22, Train Loss: 0.1301, Valid Loss: 0.0651, Time: 25.04s\n",
      "Epoch 23, Train Loss: 0.1285, Valid Loss: 0.0658, Time: 24.03s\n",
      "Epoch 24, Train Loss: 0.1283, Valid Loss: 0.0656, Time: 24.72s\n",
      "Epoch 25, Train Loss: 0.1276, Valid Loss: 0.0694, Time: 24.18s\n",
      "Epoch 26, Train Loss: 0.1269, Valid Loss: 0.0710, Time: 24.12s\n",
      "Epoch 27, Train Loss: 0.1267, Valid Loss: 0.0655, Time: 23.91s\n",
      "Epoch 28, Train Loss: 0.1262, Valid Loss: 0.0631, Time: 25.10s\n",
      "Epoch 29, Train Loss: 0.1255, Valid Loss: 0.0645, Time: 24.21s\n",
      "Epoch 30, Train Loss: 0.1251, Valid Loss: 0.0727, Time: 24.21s\n",
      "Epoch 31, Train Loss: 0.1248, Valid Loss: 0.0648, Time: 24.26s\n",
      "Epoch 32, Train Loss: 0.1248, Valid Loss: 0.0617, Time: 24.44s\n",
      "Epoch 33, Train Loss: 0.1244, Valid Loss: 0.0615, Time: 24.58s\n",
      "Epoch 34, Train Loss: 0.1236, Valid Loss: 0.0634, Time: 24.72s\n",
      "Epoch 35, Train Loss: 0.1230, Valid Loss: 0.0638, Time: 24.48s\n",
      "Epoch 36, Train Loss: 0.1228, Valid Loss: 0.0635, Time: 24.93s\n",
      "Epoch 37, Train Loss: 0.1229, Valid Loss: 0.0633, Time: 24.08s\n",
      "Epoch 38, Train Loss: 0.1222, Valid Loss: 0.0631, Time: 24.52s\n",
      "Epoch 39, Train Loss: 0.1221, Valid Loss: 0.0639, Time: 24.58s\n",
      "Epoch 40, Train Loss: 0.1222, Valid Loss: 0.0725, Time: 24.77s\n",
      "Epoch 41, Train Loss: 0.1213, Valid Loss: 0.0649, Time: 24.92s\n",
      "Epoch 42, Train Loss: 0.1215, Valid Loss: 0.0644, Time: 26.15s\n",
      "Epoch 43, Train Loss: 0.1206, Valid Loss: 0.0681, Time: 27.21s\n",
      "Epoch 44, Train Loss: 0.1204, Valid Loss: 0.0635, Time: 24.17s\n",
      "Epoch 45, Train Loss: 0.1199, Valid Loss: 0.0618, Time: 24.23s\n",
      "Epoch 46, Train Loss: 0.1202, Valid Loss: 0.0671, Time: 24.00s\n",
      "Epoch 47, Train Loss: 0.1196, Valid Loss: 0.0609, Time: 24.09s\n",
      "Epoch 48, Train Loss: 0.1196, Valid Loss: 0.0642, Time: 23.82s\n",
      "Epoch 49, Train Loss: 0.1189, Valid Loss: 0.0664, Time: 24.17s\n",
      "Epoch 50, Train Loss: 0.1191, Valid Loss: 0.0688, Time: 27.52s\n",
      "Epoch 51, Train Loss: 0.1188, Valid Loss: 0.0667, Time: 25.34s\n",
      "Epoch 52, Train Loss: 0.1184, Valid Loss: 0.0623, Time: 30.38s\n",
      "Epoch 53, Train Loss: 0.1178, Valid Loss: 0.0675, Time: 28.49s\n",
      "Epoch 54, Train Loss: 0.1178, Valid Loss: 0.0721, Time: 24.83s\n",
      "Epoch 55, Train Loss: 0.1175, Valid Loss: 0.0615, Time: 24.31s\n",
      "Epoch 56, Train Loss: 0.1175, Valid Loss: 0.0636, Time: 24.02s\n",
      "Epoch 57, Train Loss: 0.1174, Valid Loss: 0.0702, Time: 24.04s\n",
      "Epoch 58, Train Loss: 0.1172, Valid Loss: 0.0622, Time: 25.26s\n",
      "Epoch 59, Train Loss: 0.1171, Valid Loss: 0.0639, Time: 26.43s\n",
      "Epoch 60, Train Loss: 0.1168, Valid Loss: 0.0627, Time: 26.01s\n",
      "Epoch 61, Train Loss: 0.1169, Valid Loss: 0.0622, Time: 27.56s\n",
      "Epoch 62, Train Loss: 0.1166, Valid Loss: 0.0622, Time: 24.98s\n",
      "Epoch 63, Train Loss: 0.1164, Valid Loss: 0.0619, Time: 26.66s\n",
      "Epoch 64, Train Loss: 0.1150, Valid Loss: 0.0611, Time: 27.44s\n",
      "Epoch 65, Train Loss: 0.1146, Valid Loss: 0.0629, Time: 26.55s\n",
      "Epoch 66, Train Loss: 0.1146, Valid Loss: 0.0644, Time: 25.49s\n",
      "Epoch 67, Train Loss: 0.1151, Valid Loss: 0.0675, Time: 26.54s\n",
      "Epoch 68, Train Loss: 0.1148, Valid Loss: 0.0607, Time: 24.26s\n",
      "Epoch 69, Train Loss: 0.1146, Valid Loss: 0.0650, Time: 25.55s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataset, valid_dataset, dim \u001b[38;5;241m=\u001b[39m create_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# test_dataset, _ = load_test_dataset('data/test.csv')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test(valid_dataset, dim)\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataset, valid_dataset, dim, num_epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m train_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/optim/adamw.py:477\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    475\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dataset, valid_dataset, dim = create_dataset('data/train.csv')\n",
    "    # test_dataset, _ = load_test_dataset('data/test.csv')\n",
    "\n",
    "    train(train_dataset, valid_dataset, dim, num_epoch=100)\n",
    "    test(valid_dataset, dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
